2025-09-29 22:28:42,145 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 


RESOURCE_PARAMS extraction logs:
jvm_params: -Xmx536870902 -Xms536870902 -XX:MaxDirectMemorySize=268435458 -XX:MaxMetaspaceSize=268435456
dynamic_configs: -D taskmanager.memory.network.min=134217730b -D taskmanager.cpu.cores=4.0 -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.jvm-metaspace.size=268435456b -D external-resources=none -D taskmanager.memory.jvm-overhead.min=201326592b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.network.max=134217730b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=536870920b -D taskmanager.memory.task.heap.size=402653174b -D taskmanager.numberOfTaskSlots=4 -D taskmanager.memory.jvm-overhead.max=201326592b
logs: INFO  [] - Using standard YAML parser to load flink configuration file from /Users/dev/Fraud_Detection/flink-1.20.2/conf/config.yaml.
INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728m
INFO  [] - Loading configuration property: taskmanager.bind-host, localhost
INFO  [] - Loading configuration property: jobmanager.execution.failover-strategy, region
INFO  [] - Loading configuration property: jobmanager.rpc.address, localhost
INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600m
INFO  [] - Loading configuration property: jobmanager.rpc.port, 6123
INFO  [] - Loading configuration property: rest.bind-address, localhost
INFO  [] - Loading configuration property: jobmanager.bind-host, localhost
INFO  [] - Loading configuration property: taskmanager.host, localhost
INFO  [] - Loading configuration property: parallelism.default, 1
INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 4
INFO  [] - Loading configuration property: rest.address, localhost
INFO  [] - Loading configuration property: env.java.opts.all, --add-exports=java.base/sun.net.util=ALL-UNNAMED --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED
INFO  [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
INFO  [] - Final TaskExecutor Memory configuration:
INFO  [] -   Total Process Memory:          1.688gb (1811939328 bytes)
INFO  [] -     Total Flink Memory:          1.250gb (1342177280 bytes)
INFO  [] -       Total JVM Heap Memory:     512.000mb (536870902 bytes)
INFO  [] -         Framework:               128.000mb (134217728 bytes)
INFO  [] -         Task:                    384.000mb (402653174 bytes)
INFO  [] -       Total Off-heap Memory:     768.000mb (805306378 bytes)
INFO  [] -         Managed:                 512.000mb (536870920 bytes)
INFO  [] -         Total JVM Direct Memory: 256.000mb (268435458 bytes)
INFO  [] -           Framework:             128.000mb (134217728 bytes)
INFO  [] -           Task:                  0 bytes
INFO  [] -           Network:               128.000mb (134217730 bytes)
INFO  [] -     JVM Metaspace:               256.000mb (268435456 bytes)
INFO  [] -     JVM Overhead:                192.000mb (201326592 bytes)

2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Starting TaskManager (Version: 1.20.2, Scala: 2.12, Rev:1641cb9, Date:2025-06-12T21:40:37+02:00)
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  OS current user: dev
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM: OpenJDK 64-Bit Server VM - Homebrew - 17/17.0.16+0
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Arch: aarch64
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Maximum heap size: 512 MiBytes
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JAVA_HOME: /opt/homebrew/opt/openjdk@17
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  No Hadoop Dependency available
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM Options:
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xmx536870902
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xms536870902
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxDirectMemorySize=268435458
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxMetaspaceSize=268435456
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:+IgnoreUnrecognizedVMOptions
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=java.base/sun.net.util=ALL-UNNAMED
2025-09-29 22:28:42,147 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.lang=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.net=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.io=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.nio=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/sun.nio.ch=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.lang.reflect=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.text=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.time=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.util=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.util.concurrent=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog.file=/Users/dev/Fraud_Detection/flink-1.20.2/log/flink-dev-taskexecutor-0-Ds-MacBook-Air.local.log
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configuration=file:/Users/dev/Fraud_Detection/flink-1.20.2/conf/log4j.properties
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configurationFile=file:/Users/dev/Fraud_Detection/flink-1.20.2/conf/log4j.properties
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlogback.configurationFile=file:/Users/dev/Fraud_Detection/flink-1.20.2/conf/logback.xml
2025-09-29 22:28:42,148 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Program Arguments:
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --configDir
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     /Users/dev/Fraud_Detection/flink-1.20.2/conf
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.min=134217730b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.cpu.cores=4.0
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.off-heap.size=0b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-metaspace.size=268435456b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     external-resources=none
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.min=201326592b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.off-heap.size=134217728b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.max=134217730b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.heap.size=134217728b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.managed.size=536870920b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.heap.size=402653174b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.numberOfTaskSlots=4
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.max=201326592b
2025-09-29 22:28:42,149 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Classpath: /Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-cep-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-connector-files-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-connector-kafka-3.4.0-1.20.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-csv-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-json-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-scala_2.12-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-table-api-java-uber-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-table-planner-loader-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-table-runtime-1.20.2.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/kafka-clients-3.6.0.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/log4j-1.2-api-2.24.3.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/log4j-api-2.24.3.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/log4j-core-2.24.3.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/log4j-slf4j-impl-2.24.3.jar:/Users/dev/Fraud_Detection/flink-1.20.2/lib/flink-dist-1.20.2.jar::::
2025-09-29 22:28:42,150 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
2025-09-29 22:28:42,150 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Registered UNIX signal handlers for [TERM, HUP, INT]
2025-09-29 22:28:42,151 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Maximum number of open file descriptors is 1048576.
2025-09-29 22:28:42,154 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Using standard YAML parser to load flink configuration file from /Users/dev/Fraud_Detection/flink-1.20.2/conf/config.yaml.
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, localhost
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, localhost
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, localhost
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.host, localhost
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 4
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, localhost
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.all, --add-exports=java.base/sun.net.util=ALL-UNNAMED --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.min, 134217730b
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-metaspace.size, 268435456b
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.off-heap.size, 0b
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.cpu.cores, 4.0
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: external-resources, none
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.min, 201326592b
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.off-heap.size, 134217728b
2025-09-29 22:28:42,170 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.network.max, 134217730b
2025-09-29 22:28:42,171 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.framework.heap.size, 134217728b
2025-09-29 22:28:42,171 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.managed.size, 536870920b
2025-09-29 22:28:42,171 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.task.heap.size, 402653174b
2025-09-29 22:28:42,171 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.numberOfTaskSlots, 4
2025-09-29 22:28:42,171 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading dynamic configuration property: taskmanager.memory.jvm-overhead.max, 201326592b
2025-09-29 22:28:42,186 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
2025-09-29 22:28:42,193 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
2025-09-29 22:28:42,194 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
2025-09-29 22:28:42,201 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
2025-09-29 22:28:42,201 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
2025-09-29 22:28:42,202 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory,filesystem}.
2025-09-29 22:28:42,205 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
2025-09-29 22:28:42,214 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/jaas-5927294021037824212.conf.
2025-09-29 22:28:42,216 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
2025-09-29 22:28:42,333 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using configured hostname/address for TaskManager: localhost.
2025-09-29 22:28:42,347 INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils      [] - Trying to start actor system, external address localhost:0, bind address localhost:0.
2025-09-29 22:28:42,562 INFO  org.apache.pekko.event.slf4j.Slf4jLogger                     [] - Slf4jLogger started
2025-09-29 22:28:42,570 INFO  org.apache.pekko.remote.RemoteActorRefProvider               [] - Pekko Cluster not in use - enabling unsafe features anyway because `pekko.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
2025-09-29 22:28:42,571 INFO  org.apache.pekko.remote.Remoting                             [] - Starting remoting
2025-09-29 22:28:42,646 INFO  org.apache.pekko.remote.Remoting                             [] - Remoting started; listening on addresses :[pekko.tcp://flink@localhost:62144]
2025-09-29 22:28:42,678 INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils      [] - Actor system started at pekko.tcp://flink@localhost:62144
2025-09-29 22:28:42,685 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/tm_localhost:62144-ddfe5b)
2025-09-29 22:28:42,687 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2025-09-29 22:28:42,688 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No trace reporter configured, no metrics will be exposed/reported.
2025-09-29 22:28:42,689 INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils      [] - Trying to start actor system, external address localhost:0, bind address localhost:0.
2025-09-29 22:28:42,695 INFO  org.apache.pekko.event.slf4j.Slf4jLogger                     [] - Slf4jLogger started
2025-09-29 22:28:42,696 INFO  org.apache.pekko.remote.RemoteActorRefProvider               [] - Pekko Cluster not in use - enabling unsafe features anyway because `pekko.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
2025-09-29 22:28:42,696 INFO  org.apache.pekko.remote.Remoting                             [] - Starting remoting
2025-09-29 22:28:42,699 INFO  org.apache.pekko.remote.Remoting                             [] - Remoting started; listening on addresses :[pekko.tcp://flink-metrics@localhost:62145]
2025-09-29 22:28:42,702 INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils      [] - Actor system started at pekko.tcp://flink-metrics@localhost:62145
2025-09-29 22:28:42,706 INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService           [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at pekko://flink-metrics/user/rpc/MetricQueryService_localhost:62144-ddfe5b .
2025-09-29 22:28:42,710 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/tm_localhost:62144-ddfe5b/blobStorage
2025-09-29 22:28:42,712 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/tm_localhost:62144-ddfe5b/blobStorage
2025-09-29 22:28:42,712 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
2025-09-29 22:28:42,713 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
2025-09-29 22:28:42,714 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
2025-09-29 22:28:42,714 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
2025-09-29 22:28:42,714 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
2025-09-29 22:28:42,714 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
2025-09-29 22:28:42,714 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: localhost:62144-ddfe5b
2025-09-29 22:28:42,743 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T': total 228 GB, usable 37 GB (16.23% usable)
2025-09-29 22:28:42,745 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	/var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/flink-io-620b9f37-cde7-4690-9a24-5c7d7e38073d
2025-09-29 22:28:42,748 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port range: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 4 (manual), number of client threads: 4 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
2025-09-29 22:28:42,753 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	/var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/flink-netty-shuffle-b262d34b-dd7b-4620-8ea4-92ee8d6ac85b
2025-09-29 22:28:42,781 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
2025-09-29 22:28:42,787 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2025-09-29 22:28:42,791 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using NIO.
2025-09-29 22:28:42,791 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 4 ms).
2025-09-29 22:28:42,792 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using NIO.
2025-09-29 22:28:42,792 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 0 ms). Listening on SocketAddress /127.0.0.1:62146.
2025-09-29 22:28:42,792 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - TaskManager data connection initialized successfully; listening internally on port: 62146
2025-09-29 22:28:42,793 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2025-09-29 22:28:42,807 INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService           [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at pekko://flink/user/rpc/taskmanager_0 .
2025-09-29 22:28:42,814 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2025-09-29 22:28:42,815 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /var/folders/w8/tq7jb3rs56d45dstn263zg080000gn/T/flink-dist-cache-64cd9dde-b231-432f-b978-afadc2ab7541
2025-09-29 22:28:42,815 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager pekko.tcp://flink@localhost:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
2025-09-29 22:28:42,911 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2025-09-29 22:28:42,945 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager pekko.tcp://flink@localhost:6123/user/rpc/resourcemanager_* under registration id c58330be397ff7f56d1b547272976e75.
2025-09-29 22:30:58,951 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request ce406ef2db6298ed9e6990ab03a01b4c for job 7df07e5df00939a2f3081172614eb8c4 from resource manager with leader id 00000000000000000000000000000000.
2025-09-29 22:30:58,956 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Allocated slot for ce406ef2db6298ed9e6990ab03a01b4c with resources ResourceProfile{cpuCores=1, taskHeapMemory=96.000mb (100663293 bytes), taskOffHeapMemory=0 bytes, managedMemory=128.000mb (134217730 bytes), networkMemory=32.000mb (33554432 bytes)}.
2025-09-29 22:30:58,960 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 7df07e5df00939a2f3081172614eb8c4 for job leader monitoring.
2025-09-29 22:30:58,961 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
2025-09-29 22:30:58,974 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2025-09-29 22:30:58,985 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:30:58,985 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:30:58,987 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:30:59,003 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:30:59,011 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2025-09-29 22:30:59,016 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id 7df07e5df00939a2f3081172614eb8c4
2025-09-29 22:30:59,018 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:30:59,018 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
2025-09-29 22:30:59,019 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:30:59,021 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
2025-09-29 22:30:59,022 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading 7df07e5df00939a2f3081172614eb8c4/p-d47a6ec598a41f8cb5c21f3ed7444f957b096f90-8a6695c6db822304b514704c8cb919cf from localhost/127.0.0.1:62142
2025-09-29 22:30:59,065 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@454c4676
2025-09-29 22:30:59,065 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:30:59,066 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:30:59,076 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:30:59,257 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:30:59,262 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@60945d11
2025-09-29 22:30:59,272 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@471f7dec
2025-09-29 22:30:59,282 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:30:59,289 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-09-29 22:30:59,302 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:30:59,302 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:30:59,302 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165259301
2025-09-29 22:30:59,306 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@202a7c81
2025-09-29 22:30:59,306 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5b73ab9
2025-09-29 22:30:59,308 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
2025-09-29 22:30:59,317 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
2025-09-29 22:30:59,323 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:30:59,334 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:30:59,334 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:30:59,334 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:30:59,334 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165259334
2025-09-29 22:30:59,336 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:30:59,337 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:30:59,338 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to earliest offset of partition transactions-data-0
2025-09-29 22:30:59,396 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-1] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:30:59,396 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:30:59,421 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Resetting offset for partition transactions-data-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=6}}.
2025-09-29 22:30:59,509 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:30:59,509 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-09-29 22:30:59,511 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:30:59,511 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:30:59,511 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165259511
2025-09-29 22:30:59,514 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-2] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:30:59,514 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-2] ProducerId set to 2000 with epoch 0
2025-09-29 22:30:59,514 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:30:59,516 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:30:59,516 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:30:59,516 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:30:59,516 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-2 unregistered
2025-09-29 22:30:59,611 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-1] ProducerId set to 2001 with epoch 0
2025-09-29 22:30:59,668 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:30:59,723 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:30:59,726 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:30:59,727 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:30:59,727 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:30:59,727 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-1 unregistered
2025-09-29 22:30:59,727 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:30:59,727 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:00,271 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:00,271 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:00,271 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:00,273 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:00,273 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:00,274 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@572d088c).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:00,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0).
2025-09-29 22:31:00,286 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#0 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
2025-09-29 22:31:01,361 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:01,362 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:01,362 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from CREATED to DEPLOYING.
2025-09-29 22:31:01,363 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1) [DEPLOYING].
2025-09-29 22:31:01,363 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5787fe75
2025-09-29 22:31:01,363 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:01,364 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:01,364 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:01,370 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:01,370 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@72cdb32e
2025-09-29 22:31:01,374 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:01,374 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6abc0806
2025-09-29 22:31:01,375 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:01,376 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-09-29 22:31:01,378 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:01,378 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:01,378 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165261378
2025-09-29 22:31:01,378 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@56f9ee79
2025-09-29 22:31:01,378 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2122b363
2025-09-29 22:31:01,379 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:01,379 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:01,379 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:01,381 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:01,381 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:01,381 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:01,381 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165261381
2025-09-29 22:31:01,381 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-3] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:01,381 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:01,382 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:01,382 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:01,382 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:01,384 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-3] ProducerId set to 2002 with epoch 0
2025-09-29 22:31:01,387 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:01,393 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:01,393 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-09-29 22:31:01,394 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:01,394 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:01,394 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165261394
2025-09-29 22:31:01,397 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-4] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:01,397 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:01,397 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-4] ProducerId set to 2003 with epoch 0
2025-09-29 22:31:01,397 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:01,397 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:01,397 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:01,398 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-4 unregistered
2025-09-29 22:31:01,429 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:01,433 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:01,433 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:01,433 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:01,433 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-3 unregistered
2025-09-29 22:31:01,433 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:01,433 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:01,958 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:01,959 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:01,959 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:01,960 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:01,961 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:01,962 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@679c2af1).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:01,966 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1).
2025-09-29 22:31:01,969 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#1 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_1.
2025-09-29 22:31:03,447 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:03,449 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:03,449 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2) switched from CREATED to DEPLOYING.
2025-09-29 22:31:03,449 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2) [DEPLOYING].
2025-09-29 22:31:03,450 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@640f5d8d
2025-09-29 22:31:03,450 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:03,450 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:03,450 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:03,457 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:03,457 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5d083c6d
2025-09-29 22:31:03,459 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:03,459 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5256699c
2025-09-29 22:31:03,460 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:03,461 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-09-29 22:31:03,463 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:03,463 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:03,463 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165263463
2025-09-29 22:31:03,464 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7ec72e43
2025-09-29 22:31:03,464 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7b47abba
2025-09-29 22:31:03,464 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:03,464 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:03,464 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:03,466 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:03,466 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:03,466 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:03,466 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165263466
2025-09-29 22:31:03,467 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:03,467 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:03,467 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:03,468 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:03,468 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-5] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:03,471 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-5] ProducerId set to 2004 with epoch 0
2025-09-29 22:31:03,473 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:03,484 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:03,484 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-09-29 22:31:03,486 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:03,486 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:03,486 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165263486
2025-09-29 22:31:03,489 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-6] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:03,490 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-6] ProducerId set to 2005 with epoch 0
2025-09-29 22:31:03,490 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:03,490 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:03,490 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:03,490 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:03,490 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-6 unregistered
2025-09-29 22:31:03,519 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:03,526 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:03,526 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:03,526 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:03,526 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-5 unregistered
2025-09-29 22:31:03,526 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:03,526 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:04,071 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:04,072 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:04,072 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:04,074 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:04,074 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:04,076 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@65e7e351).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:04,078 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2).
2025-09-29 22:31:04,080 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#2 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_2.
2025-09-29 22:31:06,203 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:06,207 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:06,209 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3) switched from CREATED to DEPLOYING.
2025-09-29 22:31:06,210 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3) [DEPLOYING].
2025-09-29 22:31:06,214 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1e0aef02
2025-09-29 22:31:06,214 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:06,214 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:06,214 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:06,228 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:06,228 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@44e66999
2025-09-29 22:31:06,229 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:06,229 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2122b89
2025-09-29 22:31:06,230 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:06,231 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-09-29 22:31:06,233 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:06,234 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:06,234 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165266233
2025-09-29 22:31:06,235 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@27d5f282
2025-09-29 22:31:06,235 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7f1ad88d
2025-09-29 22:31:06,236 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:06,236 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:06,237 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:06,239 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:06,240 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:06,240 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:06,240 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165266239
2025-09-29 22:31:06,240 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:06,240 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:06,240 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:06,240 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:06,248 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-7] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:06,249 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:06,249 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-7] ProducerId set to 2006 with epoch 0
2025-09-29 22:31:06,257 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:06,257 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-09-29 22:31:06,258 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:06,258 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:06,258 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165266258
2025-09-29 22:31:06,260 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-8] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:06,260 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-8] ProducerId set to 2007 with epoch 0
2025-09-29 22:31:06,260 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:06,261 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:06,261 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:06,261 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:06,261 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-8 unregistered
2025-09-29 22:31:06,280 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:06,282 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:06,282 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:06,282 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:06,282 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-7 unregistered
2025-09-29 22:31:06,282 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:06,282 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:06,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:06,856 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:06,856 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:06,858 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:06,858 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:06,858 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@9c56367).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:06,859 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3).
2025-09-29 22:31:06,860 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#3 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_3.
2025-09-29 22:31:09,977 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:09,979 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:09,979 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4) switched from CREATED to DEPLOYING.
2025-09-29 22:31:09,979 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4) [DEPLOYING].
2025-09-29 22:31:09,980 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2f053655
2025-09-29 22:31:09,980 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:09,980 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:09,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:09,985 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:09,985 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@788a7be1
2025-09-29 22:31:09,986 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:09,986 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7a3ab801
2025-09-29 22:31:09,987 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:09,987 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-09-29 22:31:09,988 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:09,988 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:09,988 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165269988
2025-09-29 22:31:09,989 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@aafa65f
2025-09-29 22:31:09,989 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4b743e72
2025-09-29 22:31:09,989 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:09,990 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:09,990 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:09,991 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-9] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:09,991 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-9] ProducerId set to 2008 with epoch 0
2025-09-29 22:31:09,991 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:09,991 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:09,991 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:09,991 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165269991
2025-09-29 22:31:09,992 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:09,992 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:09,992 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:09,992 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:09,995 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:10,002 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:10,003 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-09-29 22:31:10,003 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:10,003 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:10,003 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165270003
2025-09-29 22:31:10,006 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-10] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:10,006 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-10] ProducerId set to 2009 with epoch 0
2025-09-29 22:31:10,006 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:10,006 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:10,006 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:10,006 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:10,006 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-10 unregistered
2025-09-29 22:31:10,022 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:10,024 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:10,024 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:10,024 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:10,024 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-9 unregistered
2025-09-29 22:31:10,024 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:10,024 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:10,593 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:10,594 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:10,594 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:10,596 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:10,596 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:10,597 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@4597cd10).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:10,598 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4).
2025-09-29 22:31:10,600 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#4 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_4.
2025-09-29 22:31:15,704 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:15,706 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:15,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5) switched from CREATED to DEPLOYING.
2025-09-29 22:31:15,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5) [DEPLOYING].
2025-09-29 22:31:15,708 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2f1f515c
2025-09-29 22:31:15,708 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:15,708 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:15,708 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:15,715 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:15,715 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4a7fa347
2025-09-29 22:31:15,715 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:15,715 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@41683228
2025-09-29 22:31:15,716 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:15,717 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-09-29 22:31:15,718 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:15,718 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:15,718 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165275718
2025-09-29 22:31:15,719 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@65c015ac
2025-09-29 22:31:15,719 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@52d9bea5
2025-09-29 22:31:15,719 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:15,719 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:15,719 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:15,721 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:15,721 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:15,721 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:15,721 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165275721
2025-09-29 22:31:15,721 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-11] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:15,721 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-11] ProducerId set to 2010 with epoch 0
2025-09-29 22:31:15,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:15,721 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:15,722 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:15,722 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:15,725 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:15,730 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:15,731 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-09-29 22:31:15,732 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:15,732 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:15,732 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165275732
2025-09-29 22:31:15,734 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-12] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:15,734 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-12] ProducerId set to 2011 with epoch 0
2025-09-29 22:31:15,734 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:15,735 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:15,735 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:15,735 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:15,735 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-12 unregistered
2025-09-29 22:31:15,750 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:15,754 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:15,754 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:15,754 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:15,754 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-11 unregistered
2025-09-29 22:31:15,754 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:15,754 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:16,298 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:16,298 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:16,298 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:16,300 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:16,300 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:16,301 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@70bd1ea3).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:16,301 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5).
2025-09-29 22:31:16,305 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#5 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_5.
2025-09-29 22:31:23,715 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:23,716 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:23,717 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6) switched from CREATED to DEPLOYING.
2025-09-29 22:31:23,717 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6) [DEPLOYING].
2025-09-29 22:31:23,718 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4977b53a
2025-09-29 22:31:23,718 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:23,718 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:23,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:23,724 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:23,724 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3d117bf5
2025-09-29 22:31:23,724 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:23,725 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2050fe8
2025-09-29 22:31:23,725 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:23,726 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-09-29 22:31:23,727 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:23,727 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:23,727 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165283727
2025-09-29 22:31:23,728 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@732c5a1
2025-09-29 22:31:23,728 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@77575cce
2025-09-29 22:31:23,728 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:23,729 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:23,729 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:23,730 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:23,730 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:23,730 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:23,730 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165283730
2025-09-29 22:31:23,730 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:23,730 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:23,730 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:23,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:23,749 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-13] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:23,751 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:23,751 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-13] ProducerId set to 2012 with epoch 0
2025-09-29 22:31:23,757 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:23,758 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-09-29 22:31:23,758 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:23,758 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:23,758 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165283758
2025-09-29 22:31:23,762 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-14] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:23,762 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:23,762 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-14] ProducerId set to 2013 with epoch 0
2025-09-29 22:31:23,763 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:23,763 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:23,763 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:23,763 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-14 unregistered
2025-09-29 22:31:23,777 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:23,782 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:23,782 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:23,782 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:23,782 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-13 unregistered
2025-09-29 22:31:23,782 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:23,782 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:24,327 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:24,328 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:24,328 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:24,329 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:24,329 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:24,330 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@194513e).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:24,331 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6).
2025-09-29 22:31:24,332 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#6 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_6.
2025-09-29 22:31:36,845 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:36,854 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:36,855 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7) switched from CREATED to DEPLOYING.
2025-09-29 22:31:36,856 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7) [DEPLOYING].
2025-09-29 22:31:36,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@73439d03
2025-09-29 22:31:36,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:36,863 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:36,863 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:36,875 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:36,875 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@78fd2538
2025-09-29 22:31:36,876 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:36,876 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6469248c
2025-09-29 22:31:36,877 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:36,881 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-09-29 22:31:36,887 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:36,887 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:36,887 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165296887
2025-09-29 22:31:36,889 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7a636fd8
2025-09-29 22:31:36,889 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@58f8b827
2025-09-29 22:31:36,889 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:36,889 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:36,890 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:36,893 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:36,893 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:36,893 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:36,893 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165296893
2025-09-29 22:31:36,893 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:36,894 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:36,894 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:36,894 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:36,894 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-15] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:36,895 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-15] ProducerId set to 2014 with epoch 0
2025-09-29 22:31:36,900 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:36,974 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:37,062 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:37,063 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-09-29 22:31:37,066 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:37,066 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:37,066 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165297066
2025-09-29 22:31:37,069 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-16] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:37,069 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-16] ProducerId set to 2015 with epoch 0
2025-09-29 22:31:37,069 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:37,069 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:37,069 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:37,070 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:37,070 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-16 unregistered
2025-09-29 22:31:37,133 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:37,138 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:37,138 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:37,138 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:37,138 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-15 unregistered
2025-09-29 22:31:37,138 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:37,138 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:37,692 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:37,692 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:37,693 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:37,694 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:37,694 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:37,694 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@73149e16).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:37,695 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7).
2025-09-29 22:31:37,695 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#7 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_7.
2025-09-29 22:31:54,651 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:54,653 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:31:54,654 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8) switched from CREATED to DEPLOYING.
2025-09-29 22:31:54,655 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8) [DEPLOYING].
2025-09-29 22:31:54,656 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@20daaac4
2025-09-29 22:31:54,656 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:31:54,656 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:31:54,656 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:31:54,662 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:31:54,662 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@70f66332
2025-09-29 22:31:54,663 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:54,663 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4b7620f7
2025-09-29 22:31:54,664 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:54,664 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-09-29 22:31:54,668 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:54,668 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:54,668 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165314668
2025-09-29 22:31:54,670 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2c4e2de1
2025-09-29 22:31:54,670 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@dd133a0
2025-09-29 22:31:54,670 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:31:54,670 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:31:54,671 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:31:54,672 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:31:54,672 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:54,672 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:54,672 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165314672
2025-09-29 22:31:54,673 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:31:54,673 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8) switched from INITIALIZING to RUNNING.
2025-09-29 22:31:54,673 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:31:54,673 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:31:54,676 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-17] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:54,678 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-17] ProducerId set to 2016 with epoch 0
2025-09-29 22:31:54,678 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:54,788 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:31:54,873 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:31:54,873 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-09-29 22:31:54,874 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:31:54,874 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:31:54,874 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165314874
2025-09-29 22:31:54,877 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-18] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:31:54,877 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:31:54,877 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-18] ProducerId set to 2017 with epoch 0
2025-09-29 22:31:54,878 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:54,878 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:54,878 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:54,879 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-18 unregistered
2025-09-29 22:31:54,896 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:31:54,913 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:54,913 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:54,913 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:54,913 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-17 unregistered
2025-09-29 22:31:54,913 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:31:54,913 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:31:55,440 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:31:55,440 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:31:55,441 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:31:55,441 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:31:55,442 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:31:55,442 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@754db2b8).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:31:55,443 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8).
2025-09-29 22:31:55,444 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#8 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_8.
2025-09-29 22:32:19,939 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:32:19,947 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:32:19,948 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9) switched from CREATED to DEPLOYING.
2025-09-29 22:32:19,948 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9) [DEPLOYING].
2025-09-29 22:32:19,949 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@68809d2c
2025-09-29 22:32:19,949 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:32:19,949 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:32:19,950 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:32:19,956 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:32:19,957 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3a2f7488
2025-09-29 22:32:19,959 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:32:19,959 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2e249f9
2025-09-29 22:32:19,960 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:32:19,961 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-09-29 22:32:19,964 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:32:19,964 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:32:19,964 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165339964
2025-09-29 22:32:19,964 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@68cc24d8
2025-09-29 22:32:19,964 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@29d9318
2025-09-29 22:32:19,965 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:32:19,965 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:32:19,965 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:32:19,966 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:32:19,966 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:32:19,966 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:32:19,966 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165339966
2025-09-29 22:32:19,967 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:32:19,967 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9) switched from INITIALIZING to RUNNING.
2025-09-29 22:32:19,967 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:32:19,967 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:32:19,968 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-19] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:32:19,969 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-19] ProducerId set to 2018 with epoch 0
2025-09-29 22:32:19,971 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:32:19,984 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:32:19,984 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-09-29 22:32:19,985 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:32:19,985 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:32:19,985 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165339985
2025-09-29 22:32:19,988 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-20] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:32:19,988 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-20] ProducerId set to 2019 with epoch 0
2025-09-29 22:32:19,988 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:32:19,988 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:32:19,988 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:32:19,988 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:32:19,989 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-20 unregistered
2025-09-29 22:32:20,007 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:32:20,010 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:32:20,010 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:32:20,010 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:32:20,011 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-19 unregistered
2025-09-29 22:32:20,011 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:32:20,011 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:32:20,528 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:32:20,528 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:32:20,529 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:32:20,530 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:32:20,530 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:32:20,531 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@43352eff).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:32:20,532 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9).
2025-09-29 22:32:20,533 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#9 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_9.
2025-09-29 22:32:59,032 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:32:59,035 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:32:59,035 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10) switched from CREATED to DEPLOYING.
2025-09-29 22:32:59,036 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10) [DEPLOYING].
2025-09-29 22:32:59,037 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@45dc388f
2025-09-29 22:32:59,037 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:32:59,037 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:32:59,037 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:32:59,045 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:32:59,045 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6c34f106
2025-09-29 22:32:59,046 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:32:59,046 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3cbcb5e5
2025-09-29 22:32:59,047 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:32:59,048 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-09-29 22:32:59,050 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:32:59,050 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:32:59,050 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165379050
2025-09-29 22:32:59,050 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@47ff32d1
2025-09-29 22:32:59,050 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3dcca630
2025-09-29 22:32:59,051 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:32:59,051 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:32:59,051 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:32:59,052 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:32:59,052 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:32:59,052 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:32:59,052 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165379052
2025-09-29 22:32:59,052 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:32:59,053 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10) switched from INITIALIZING to RUNNING.
2025-09-29 22:32:59,053 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:32:59,053 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:32:59,055 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-21] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:32:59,056 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-21] ProducerId set to 2020 with epoch 0
2025-09-29 22:32:59,056 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:32:59,061 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:32:59,061 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-09-29 22:32:59,062 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:32:59,062 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:32:59,062 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165379062
2025-09-29 22:32:59,064 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-22] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:32:59,064 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-22] ProducerId set to 2021 with epoch 0
2025-09-29 22:32:59,064 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:32:59,064 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:32:59,064 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:32:59,064 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:32:59,064 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-22 unregistered
2025-09-29 22:32:59,094 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:32:59,096 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:32:59,096 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:32:59,096 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:32:59,096 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-21 unregistered
2025-09-29 22:32:59,097 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:32:59,097 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:32:59,644 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:32:59,644 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:32:59,644 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:32:59,646 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:32:59,646 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:32:59,647 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@19582440).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:32:59,648 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10).
2025-09-29 22:32:59,649 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#10 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_10.
2025-09-29 22:33:59,713 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:33:59,721 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:33:59,728 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11) switched from CREATED to DEPLOYING.
2025-09-29 22:33:59,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11) [DEPLOYING].
2025-09-29 22:33:59,732 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6aa667cf
2025-09-29 22:33:59,732 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:33:59,732 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:33:59,733 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:33:59,759 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:33:59,760 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5d0b2a65
2025-09-29 22:33:59,760 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:33:59,760 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5fbc21bd
2025-09-29 22:33:59,762 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:33:59,763 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-09-29 22:33:59,775 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:33:59,775 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:33:59,776 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165439775
2025-09-29 22:33:59,780 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5c658857
2025-09-29 22:33:59,780 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@39c1c4e7
2025-09-29 22:33:59,780 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:33:59,780 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:33:59,781 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:33:59,783 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-23] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:33:59,784 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-23] ProducerId set to 2022 with epoch 0
2025-09-29 22:33:59,792 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:33:59,792 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:33:59,792 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:33:59,792 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165439792
2025-09-29 22:33:59,793 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:33:59,793 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:33:59,793 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:33:59,793 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11) switched from INITIALIZING to RUNNING.
2025-09-29 22:33:59,800 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:33:59,893 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:33:59,893 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-24] Instantiated an idempotent producer.
2025-09-29 22:33:59,897 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:33:59,897 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:33:59,897 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165439897
2025-09-29 22:33:59,902 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-24] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:33:59,902 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-24] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:33:59,903 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-24] ProducerId set to 2023 with epoch 0
2025-09-29 22:33:59,911 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:33:59,911 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:33:59,911 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:33:59,911 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-24 unregistered
2025-09-29 22:34:00,082 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-23] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:34:00,084 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:34:00,084 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:34:00,084 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:34:00,084 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-23 unregistered
2025-09-29 22:34:00,085 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:34:00,085 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:34:00,632 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:34:00,633 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:34:00,634 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:34:00,636 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:34:00,636 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:34:00,638 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@53b3db8b).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:34:00,640 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11).
2025-09-29 22:34:00,642 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#11 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_11.
2025-09-29 22:35:00,706 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:35:00,713 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12), deploy into slot with allocation id ce406ef2db6298ed9e6990ab03a01b4c.
2025-09-29 22:35:00,714 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12) switched from CREATED to DEPLOYING.
2025-09-29 22:35:00,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12) [DEPLOYING].
2025-09-29 22:35:00,719 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@177b6b20
2025-09-29 22:35:00,719 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:35:00,719 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:35:00,721 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:35:00,730 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:35:00,730 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@344cc4a9
2025-09-29 22:35:00,731 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:35:00,731 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@59514786
2025-09-29 22:35:00,733 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:35:00,734 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-25] Instantiated an idempotent producer.
2025-09-29 22:35:00,737 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:35:00,737 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:35:00,737 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165500737
2025-09-29 22:35:00,738 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4820d82b
2025-09-29 22:35:00,738 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3d0c5eb9
2025-09-29 22:35:00,738 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:35:00,738 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:35:00,739 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:35:00,740 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:35:00,740 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:35:00,740 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:35:00,740 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165500740
2025-09-29 22:35:00,741 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:35:00,741 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:35:00,741 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:35:00,741 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12) switched from INITIALIZING to RUNNING.
2025-09-29 22:35:00,751 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-25] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:35:00,751 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:35:00,752 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-25] ProducerId set to 2024 with epoch 0
2025-09-29 22:35:00,768 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:35:00,768 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-26] Instantiated an idempotent producer.
2025-09-29 22:35:00,768 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:35:00,768 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:35:00,768 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165500768
2025-09-29 22:35:00,771 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-26] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:35:00,771 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-26] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:35:00,771 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-26] ProducerId set to 2025 with epoch 0
2025-09-29 22:35:00,772 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:35:00,772 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:35:00,772 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:35:00,772 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-26 unregistered
2025-09-29 22:35:00,790 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-25] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:35:00,794 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:35:00,794 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:35:00,794 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:35:00,794 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-25 unregistered
2025-09-29 22:35:00,794 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:35:00,794 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:35:01,292 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:35:01,293 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:35:01,293 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:35:01,295 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:35:01,295 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:35:01,296 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@155f62c5).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:35:01,298 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12).
2025-09-29 22:35:01,299 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#12 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_12.
2025-09-29 22:35:58,908 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:4, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=96.000mb (100663293 bytes), taskOffHeapMemory=0 bytes, managedMemory=128.000mb (134217730 bytes), networkMemory=32.000mb (33554432 bytes)}, allocationId: ce406ef2db6298ed9e6990ab03a01b4c, jobId: 7df07e5df00939a2f3081172614eb8c4).
2025-09-29 22:35:58,917 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 7df07e5df00939a2f3081172614eb8c4 from job leader monitoring.
2025-09-29 22:35:58,918 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:36:01,420 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 08b2e2d58b632478e4d2d7538ff5a572 for job 7df07e5df00939a2f3081172614eb8c4 from resource manager with leader id 00000000000000000000000000000000.
2025-09-29 22:36:01,423 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Allocated slot for 08b2e2d58b632478e4d2d7538ff5a572 with resources ResourceProfile{cpuCores=1, taskHeapMemory=96.000mb (100663293 bytes), taskOffHeapMemory=0 bytes, managedMemory=128.000mb (134217730 bytes), networkMemory=32.000mb (33554432 bytes)}.
2025-09-29 22:36:01,423 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 7df07e5df00939a2f3081172614eb8c4 for job leader monitoring.
2025-09-29 22:36:01,424 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
2025-09-29 22:36:01,436 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2025-09-29 22:36:01,455 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:36:01,456 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:36:01,456 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:36:01,461 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:36:01,463 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:36:01,464 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2025-09-29 22:36:01,464 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id 7df07e5df00939a2f3081172614eb8c4
2025-09-29 22:36:01,464 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13), deploy into slot with allocation id 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:36:01,465 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13) switched from CREATED to DEPLOYING.
2025-09-29 22:36:01,465 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13) [DEPLOYING].
2025-09-29 22:36:01,466 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2020ddb5
2025-09-29 22:36:01,466 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:36:01,466 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:36:01,467 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:36:01,486 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:36:01,486 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@628c762f
2025-09-29 22:36:01,487 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:36:01,487 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4838d94c
2025-09-29 22:36:01,488 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:36:01,489 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-27] Instantiated an idempotent producer.
2025-09-29 22:36:01,491 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:36:01,491 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:36:01,491 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165561490
2025-09-29 22:36:01,491 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3372eb31
2025-09-29 22:36:01,491 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@70fb60f0
2025-09-29 22:36:01,491 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:36:01,491 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:36:01,491 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:36:01,493 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:36:01,493 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:36:01,493 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:36:01,493 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165561493
2025-09-29 22:36:01,493 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:36:01,494 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:36:01,494 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13) switched from INITIALIZING to RUNNING.
2025-09-29 22:36:01,494 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:36:01,499 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:36:01,499 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-27] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:36:01,499 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-27] ProducerId set to 2026 with epoch 0
2025-09-29 22:36:01,504 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:36:01,504 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-28] Instantiated an idempotent producer.
2025-09-29 22:36:01,505 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:36:01,505 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:36:01,505 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165561505
2025-09-29 22:36:01,507 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-28] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:36:01,507 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-28] ProducerId set to 2027 with epoch 0
2025-09-29 22:36:01,507 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-28] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:36:01,507 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:36:01,507 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:36:01,507 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:36:01,507 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-28 unregistered
2025-09-29 22:36:01,528 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-27] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:36:01,530 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:36:01,530 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:36:01,530 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:36:01,531 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-27 unregistered
2025-09-29 22:36:01,531 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:36:01,531 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:36:02,089 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:36:02,089 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:36:02,090 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:36:02,092 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:36:02,092 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:36:02,092 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@283e381e).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:36:02,094 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13).
2025-09-29 22:36:02,097 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#13 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_13.
2025-09-29 22:37:02,150 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:37:02,156 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14), deploy into slot with allocation id 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:37:02,157 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14) switched from CREATED to DEPLOYING.
2025-09-29 22:37:02,160 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14) [DEPLOYING].
2025-09-29 22:37:02,161 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2ab113fe
2025-09-29 22:37:02,161 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:37:02,162 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:37:02,162 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:37:02,169 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:37:02,169 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@781413b6
2025-09-29 22:37:02,170 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:37:02,170 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@126dfd4f
2025-09-29 22:37:02,171 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:37:02,172 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-29] Instantiated an idempotent producer.
2025-09-29 22:37:02,174 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:37:02,174 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:37:02,174 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165622174
2025-09-29 22:37:02,175 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4d380f49
2025-09-29 22:37:02,175 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5ca35ad7
2025-09-29 22:37:02,175 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:37:02,175 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:37:02,176 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:37:02,177 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:37:02,177 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:37:02,177 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:37:02,177 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165622177
2025-09-29 22:37:02,177 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:37:02,178 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:37:02,178 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:37:02,178 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14) switched from INITIALIZING to RUNNING.
2025-09-29 22:37:02,182 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-29] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:37:02,182 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:37:02,182 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-29] ProducerId set to 2028 with epoch 0
2025-09-29 22:37:02,189 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:37:02,189 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-30] Instantiated an idempotent producer.
2025-09-29 22:37:02,190 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:37:02,190 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:37:02,190 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165622190
2025-09-29 22:37:02,191 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-30] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:37:02,192 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-30] ProducerId set to 2029 with epoch 0
2025-09-29 22:37:02,192 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-30] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:37:02,192 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:37:02,192 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:37:02,192 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:37:02,192 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-30 unregistered
2025-09-29 22:37:02,210 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-29] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:37:02,213 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:37:02,213 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:37:02,213 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:37:02,213 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-29 unregistered
2025-09-29 22:37:02,213 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:37:02,213 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:37:02,763 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:37:02,764 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:37:02,764 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:37:02,765 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:37:02,766 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:37:02,766 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@74028ca8).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:37:02,769 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14).
2025-09-29 22:37:02,770 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#14 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_14.
2025-09-29 22:38:02,813 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:38:02,817 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15), deploy into slot with allocation id 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:38:02,818 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15) switched from CREATED to DEPLOYING.
2025-09-29 22:38:02,821 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15) [DEPLOYING].
2025-09-29 22:38:02,823 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2d5de004
2025-09-29 22:38:02,823 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:38:02,823 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:38:02,824 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:38:02,831 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:38:02,832 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@76df07f8
2025-09-29 22:38:02,833 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:38:02,833 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7a61bd38
2025-09-29 22:38:02,835 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:38:02,836 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-31] Instantiated an idempotent producer.
2025-09-29 22:38:02,838 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:38:02,838 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:38:02,838 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165682838
2025-09-29 22:38:02,838 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5d62771a
2025-09-29 22:38:02,838 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1870d3
2025-09-29 22:38:02,838 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:38:02,838 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:38:02,839 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:38:02,840 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:38:02,840 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:38:02,840 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:38:02,840 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165682840
2025-09-29 22:38:02,841 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:38:02,842 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15) switched from INITIALIZING to RUNNING.
2025-09-29 22:38:02,842 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:38:02,842 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:38:02,843 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-31] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:38:02,844 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-31] ProducerId set to 2030 with epoch 0
2025-09-29 22:38:02,845 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:38:02,851 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:38:02,851 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-32] Instantiated an idempotent producer.
2025-09-29 22:38:02,852 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:38:02,852 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:38:02,852 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165682852
2025-09-29 22:38:02,854 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-32] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:38:02,855 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-32] ProducerId set to 2031 with epoch 0
2025-09-29 22:38:02,855 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-32] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:38:02,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:38:02,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:38:02,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:38:02,855 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-32 unregistered
2025-09-29 22:38:02,874 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-31] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:38:02,877 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:38:02,877 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:38:02,877 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:38:02,877 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-31 unregistered
2025-09-29 22:38:02,877 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:38:02,877 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:38:03,431 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:38:03,431 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:38:03,431 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:38:03,433 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:38:03,433 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:38:03,433 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@10c3ad40).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:38:03,435 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15).
2025-09-29 22:38:03,437 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#15 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_15.
2025-09-29 22:39:03,496 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:39:03,498 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16), deploy into slot with allocation id 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:39:03,499 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16) switched from CREATED to DEPLOYING.
2025-09-29 22:39:03,499 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16) [DEPLOYING].
2025-09-29 22:39:03,500 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@26e1c8f7
2025-09-29 22:39:03,500 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:39:03,500 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:39:03,500 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:39:03,507 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:39:03,508 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@59141382
2025-09-29 22:39:03,509 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:39:03,509 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7dfac37d
2025-09-29 22:39:03,510 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:39:03,511 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-33] Instantiated an idempotent producer.
2025-09-29 22:39:03,513 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:39:03,513 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:39:03,513 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165743513
2025-09-29 22:39:03,514 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3c833a99
2025-09-29 22:39:03,514 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@447a0cf4
2025-09-29 22:39:03,514 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:39:03,514 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:39:03,515 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:39:03,518 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:39:03,518 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:39:03,518 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:39:03,518 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165743518
2025-09-29 22:39:03,519 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:39:03,519 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:39:03,519 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:39:03,519 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16) switched from INITIALIZING to RUNNING.
2025-09-29 22:39:03,524 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-33] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:39:03,526 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-33] ProducerId set to 2032 with epoch 0
2025-09-29 22:39:03,527 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:39:03,534 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-34
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:39:03,534 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-34] Instantiated an idempotent producer.
2025-09-29 22:39:03,535 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:39:03,535 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:39:03,535 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165743535
2025-09-29 22:39:03,537 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-34] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:39:03,537 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-34] ProducerId set to 2033 with epoch 0
2025-09-29 22:39:03,537 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-34] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:39:03,538 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:39:03,538 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:39:03,538 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:39:03,538 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-34 unregistered
2025-09-29 22:39:03,561 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-33] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:39:03,562 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:39:03,562 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:39:03,562 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:39:03,563 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-33 unregistered
2025-09-29 22:39:03,563 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:39:03,563 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:39:04,085 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:39:04,085 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:39:04,085 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:39:04,087 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:39:04,087 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:39:04,087 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@5676f253).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:39:04,089 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16).
2025-09-29 22:39:04,090 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#16 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_16.
2025-09-29 22:40:04,131 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:40:04,135 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17), deploy into slot with allocation id 08b2e2d58b632478e4d2d7538ff5a572.
2025-09-29 22:40:04,135 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17) switched from CREATED to DEPLOYING.
2025-09-29 22:40:04,137 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17) [DEPLOYING].
2025-09-29 22:40:04,138 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@24c8e9a4
2025-09-29 22:40:04,138 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:40:04,138 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:40:04,138 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:40:04,144 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:40:04,144 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@47797053
2025-09-29 22:40:04,145 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:40:04,145 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6b1390f5
2025-09-29 22:40:04,146 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-35
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:40:04,146 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-35] Instantiated an idempotent producer.
2025-09-29 22:40:04,150 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:40:04,150 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:40:04,150 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165804150
2025-09-29 22:40:04,151 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@35e39a6e
2025-09-29 22:40:04,151 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2ada96d6
2025-09-29 22:40:04,152 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:40:04,152 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:40:04,153 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:40:04,154 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:40:04,154 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:40:04,154 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:40:04,154 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165804154
2025-09-29 22:40:04,155 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:40:04,155 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17) switched from INITIALIZING to RUNNING.
2025-09-29 22:40:04,155 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:40:04,155 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:40:04,158 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-35] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:40:04,158 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-35] ProducerId set to 2034 with epoch 0
2025-09-29 22:40:04,159 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:40:04,169 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-36
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:40:04,169 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-36] Instantiated an idempotent producer.
2025-09-29 22:40:04,170 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:40:04,170 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:40:04,170 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165804170
2025-09-29 22:40:04,174 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-36] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:40:04,174 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-36] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:40:04,174 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-36] ProducerId set to 2035 with epoch 0
2025-09-29 22:40:04,175 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:40:04,175 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:40:04,175 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:40:04,175 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-36 unregistered
2025-09-29 22:40:04,223 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-35] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:40:04,226 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:40:04,226 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:40:04,226 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:40:04,226 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-35 unregistered
2025-09-29 22:40:04,226 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:40:04,226 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:40:04,766 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:40:04,766 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:40:04,766 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:40:04,767 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:40:04,767 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:40:04,767 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@56ac0461).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:40:04,768 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17).
2025-09-29 22:40:04,769 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#17 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_17.
2025-09-29 22:40:58,944 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:5, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=96.000mb (100663293 bytes), taskOffHeapMemory=0 bytes, managedMemory=128.000mb (134217730 bytes), networkMemory=32.000mb (33554432 bytes)}, allocationId: 08b2e2d58b632478e4d2d7538ff5a572, jobId: 7df07e5df00939a2f3081172614eb8c4).
2025-09-29 22:40:58,947 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 7df07e5df00939a2f3081172614eb8c4 from job leader monitoring.
2025-09-29 22:40:58,947 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:41:04,882 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request ae3d5b5b52247560d5a5d365049546da for job 7df07e5df00939a2f3081172614eb8c4 from resource manager with leader id 00000000000000000000000000000000.
2025-09-29 22:41:04,884 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Allocated slot for ae3d5b5b52247560d5a5d365049546da with resources ResourceProfile{cpuCores=1, taskHeapMemory=96.000mb (100663293 bytes), taskOffHeapMemory=0 bytes, managedMemory=128.000mb (134217730 bytes), networkMemory=32.000mb (33554432 bytes)}.
2025-09-29 22:41:04,885 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 7df07e5df00939a2f3081172614eb8c4 for job leader monitoring.
2025-09-29 22:41:04,885 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
2025-09-29 22:41:04,899 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2025-09-29 22:41:04,910 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:41:04,910 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:41:04,910 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 7df07e5df00939a2f3081172614eb8c4.
2025-09-29 22:41:04,915 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:41:04,919 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:41:04,920 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2025-09-29 22:41:04,920 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id 7df07e5df00939a2f3081172614eb8c4
2025-09-29 22:41:04,921 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18), deploy into slot with allocation id ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:41:04,921 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18) switched from CREATED to DEPLOYING.
2025-09-29 22:41:04,922 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18) [DEPLOYING].
2025-09-29 22:41:04,927 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@f5840e0
2025-09-29 22:41:04,927 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:41:04,927 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:41:04,927 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:41:04,957 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:41:04,957 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@e57a6a
2025-09-29 22:41:04,959 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:41:04,959 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@49531054
2025-09-29 22:41:04,960 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-37
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:41:04,960 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-37] Instantiated an idempotent producer.
2025-09-29 22:41:04,962 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:41:04,962 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:41:04,962 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165864962
2025-09-29 22:41:04,962 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1d61ffe1
2025-09-29 22:41:04,962 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@227e1fa1
2025-09-29 22:41:04,962 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:41:04,962 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:41:04,963 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:41:04,964 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:41:04,964 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:41:04,964 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:41:04,964 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165864964
2025-09-29 22:41:04,964 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:41:04,964 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:41:04,964 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:41:04,965 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18) switched from INITIALIZING to RUNNING.
2025-09-29 22:41:04,968 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-37] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:41:04,968 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:41:04,968 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-37] ProducerId set to 2036 with epoch 0
2025-09-29 22:41:04,976 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-38
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:41:04,976 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-38] Instantiated an idempotent producer.
2025-09-29 22:41:04,977 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:41:04,977 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:41:04,977 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165864977
2025-09-29 22:41:04,980 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-38] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:41:04,980 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-38] ProducerId set to 2037 with epoch 0
2025-09-29 22:41:04,980 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-38] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:41:04,981 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:41:04,981 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:41:04,981 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:41:04,981 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-38 unregistered
2025-09-29 22:41:04,998 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-37] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:41:04,999 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:41:04,999 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:41:04,999 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:41:04,999 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-37 unregistered
2025-09-29 22:41:04,999 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:41:04,999 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:41:05,551 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:41:05,552 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:41:05,552 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:41:05,553 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:41:05,553 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:41:05,554 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@3e8fa083).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:41:05,555 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18).
2025-09-29 22:41:05,557 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#18 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_18.
2025-09-29 22:42:05,601 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:42:05,606 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19), deploy into slot with allocation id ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:42:05,606 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19) switched from CREATED to DEPLOYING.
2025-09-29 22:42:05,607 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19) [DEPLOYING].
2025-09-29 22:42:05,609 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@27bc8833
2025-09-29 22:42:05,609 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:42:05,609 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:42:05,609 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:42:05,622 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:42:05,622 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7bb96604
2025-09-29 22:42:05,623 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:42:05,623 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@11d70a3d
2025-09-29 22:42:05,625 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-39
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:42:05,626 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-39] Instantiated an idempotent producer.
2025-09-29 22:42:05,628 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:42:05,628 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:42:05,628 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165925628
2025-09-29 22:42:05,629 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4ac969f8
2025-09-29 22:42:05,629 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1df25072
2025-09-29 22:42:05,629 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:42:05,629 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:42:05,630 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:42:05,632 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:42:05,632 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:42:05,632 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:42:05,632 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165925632
2025-09-29 22:42:05,633 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:42:05,634 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19) switched from INITIALIZING to RUNNING.
2025-09-29 22:42:05,635 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:42:05,635 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:42:05,639 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-39] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:42:05,639 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:42:05,639 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-39] ProducerId set to 2038 with epoch 0
2025-09-29 22:42:05,650 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-40
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:42:05,650 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-40] Instantiated an idempotent producer.
2025-09-29 22:42:05,651 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:42:05,651 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:42:05,651 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165925651
2025-09-29 22:42:05,653 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-40] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:42:05,653 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-40] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:42:05,653 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-40] ProducerId set to 2039 with epoch 0
2025-09-29 22:42:05,654 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:42:05,654 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:42:05,654 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:42:05,654 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-40 unregistered
2025-09-29 22:42:05,694 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-39] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:42:05,696 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:42:05,696 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:42:05,696 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:42:05,696 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-39 unregistered
2025-09-29 22:42:05,696 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:42:05,696 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:42:06,254 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:42:06,255 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:42:06,255 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:42:06,257 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:42:06,258 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:42:06,259 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@46f33ac).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:42:06,263 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19).
2025-09-29 22:42:06,268 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#19 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_19.
2025-09-29 22:43:06,352 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:43:06,359 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20), deploy into slot with allocation id ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:43:06,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20) switched from CREATED to DEPLOYING.
2025-09-29 22:43:06,362 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20) [DEPLOYING].
2025-09-29 22:43:06,363 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1eb9f6aa
2025-09-29 22:43:06,363 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:43:06,363 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:43:06,363 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:43:06,371 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:43:06,372 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@481d204b
2025-09-29 22:43:06,376 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:43:06,376 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6ddc688f
2025-09-29 22:43:06,377 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-41
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:43:06,378 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-41] Instantiated an idempotent producer.
2025-09-29 22:43:06,381 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:43:06,381 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:43:06,381 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165986381
2025-09-29 22:43:06,382 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7dff3851
2025-09-29 22:43:06,382 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@32589c65
2025-09-29 22:43:06,382 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:43:06,382 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:43:06,382 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:43:06,384 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:43:06,384 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:43:06,384 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:43:06,384 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165986384
2025-09-29 22:43:06,384 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:43:06,384 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:43:06,385 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:43:06,385 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20) switched from INITIALIZING to RUNNING.
2025-09-29 22:43:06,387 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-41] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:43:06,388 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-41] ProducerId set to 2040 with epoch 0
2025-09-29 22:43:06,392 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:43:06,405 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-42
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:43:06,405 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-42] Instantiated an idempotent producer.
2025-09-29 22:43:06,406 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:43:06,406 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:43:06,406 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759165986406
2025-09-29 22:43:06,409 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-42] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:43:06,409 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-42] ProducerId set to 2041 with epoch 0
2025-09-29 22:43:06,409 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-42] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:43:06,409 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:43:06,409 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:43:06,409 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:43:06,410 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-42 unregistered
2025-09-29 22:43:06,454 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-41] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:43:06,462 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:43:06,463 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:43:06,463 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:43:06,463 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-41 unregistered
2025-09-29 22:43:06,463 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:43:06,463 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:43:07,014 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:43:07,014 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:43:07,014 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:43:07,016 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:43:07,016 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:43:07,016 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@18e4051a).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:43:07,017 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20).
2025-09-29 22:43:07,018 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#20 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_20.
2025-09-29 22:44:07,072 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:44:07,077 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21), deploy into slot with allocation id ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:44:07,078 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21) switched from CREATED to DEPLOYING.
2025-09-29 22:44:07,079 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21) [DEPLOYING].
2025-09-29 22:44:07,080 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5fce17c4
2025-09-29 22:44:07,080 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:44:07,080 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:44:07,080 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:44:07,090 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:44:07,090 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1b73defa
2025-09-29 22:44:07,091 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:44:07,091 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@10d89292
2025-09-29 22:44:07,092 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-43
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:44:07,093 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-43] Instantiated an idempotent producer.
2025-09-29 22:44:07,096 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:44:07,096 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:44:07,096 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759166047096
2025-09-29 22:44:07,096 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@15642abd
2025-09-29 22:44:07,096 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2916bab5
2025-09-29 22:44:07,096 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:44:07,097 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:44:07,097 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:44:07,099 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:44:07,099 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:44:07,099 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:44:07,099 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759166047099
2025-09-29 22:44:07,100 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:44:07,100 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:44:07,100 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21) switched from INITIALIZING to RUNNING.
2025-09-29 22:44:07,100 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:44:07,104 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-43] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:44:07,105 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-43] ProducerId set to 2042 with epoch 0
2025-09-29 22:44:07,105 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:44:07,115 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-44
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:44:07,115 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-44] Instantiated an idempotent producer.
2025-09-29 22:44:07,116 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:44:07,116 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:44:07,116 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759166047116
2025-09-29 22:44:07,119 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-44] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:44:07,119 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-44] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:44:07,120 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-44] ProducerId set to 2043 with epoch 0
2025-09-29 22:44:07,120 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:44:07,120 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:44:07,120 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:44:07,120 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-44 unregistered
2025-09-29 22:44:07,141 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-43] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:44:07,155 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:44:07,155 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:44:07,155 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:44:07,155 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-43 unregistered
2025-09-29 22:44:07,155 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:44:07,155 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:44:07,659 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:44:07,659 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:44:07,660 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:44:07,661 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:44:07,661 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:44:07,662 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@46bc7a66).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:44:07,664 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21).
2025-09-29 22:44:07,665 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#21 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_21.
2025-09-29 22:45:07,721 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:45:07,724 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22), deploy into slot with allocation id ae3d5b5b52247560d5a5d365049546da.
2025-09-29 22:45:07,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22) switched from CREATED to DEPLOYING.
2025-09-29 22:45:07,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22) [DEPLOYING].
2025-09-29 22:45:07,728 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@648f7864
2025-09-29 22:45:07,728 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-09-29 22:45:07,728 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2025-09-29 22:45:07,729 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22) switched from DEPLOYING to INITIALIZING.
2025-09-29 22:45:07,735 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Offset commit on checkpoint is disabled. Consuming offset will not be reported back to Kafka cluster.
2025-09-29 22:45:07,736 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@379a3de3
2025-09-29 22:45:07,738 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'pendingCommittables'. Metric will not be reported.[localhost, taskmanager, localhost:62144-ddfe5b, insert-into_default_catalog.default_database.alerts, alerts[3]: Committer, 0]
2025-09-29 22:45:07,738 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@36d0f0c7
2025-09-29 22:45:07,740 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-45
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:45:07,741 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-45] Instantiated an idempotent producer.
2025-09-29 22:45:07,742 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:45:07,742 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:45:07,742 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759166107742
2025-09-29 22:45:07,743 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@8c10963
2025-09-29 22:45:07,743 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend is set to heap memory org.apache.flink.runtime.state.hashmap.HashMapStateBackend@518b6f3b
2025-09-29 22:45:07,743 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Restoring state for 1 split(s) to reader.
2025-09-29 22:45:07,743 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: transactions-data-0, StartingOffset: 4716, StoppingOffset: -9223372036854775808]]
2025-09-29 22:45:07,743 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = KafkaSource-1629489345713191071-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2025-09-29 22:45:07,744 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - These configurations '[commit.offsets.on.checkpoint, client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
2025-09-29 22:45:07,744 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:45:07,744 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:45:07,744 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759166107744
2025-09-29 22:45:07,744 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2025-09-29 22:45:07,745 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22) switched from INITIALIZING to RUNNING.
2025-09-29 22:45:07,745 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Assigned to partition(s): transactions-data-0
2025-09-29 22:45:07,746 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Seeking to offset 4716 for partition transactions-data-0
2025-09-29 22:45:07,751 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=KafkaSource-1629489345713191071-0, groupId=null] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:45:07,751 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-45] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:45:07,752 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-45] ProducerId set to 2044 with epoch 0
2025-09-29 22:45:07,766 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-46
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2025-09-29 22:45:07,766 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-46] Instantiated an idempotent producer.
2025-09-29 22:45:07,767 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 3.6.0
2025-09-29 22:45:07,767 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: 60e845626d8a465a
2025-09-29 22:45:07,767 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1759166107767
2025-09-29 22:45:07,769 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-46] Cluster ID: q3xk7deyQhC2vrswHioXSw
2025-09-29 22:45:07,769 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-46] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-09-29 22:45:07,770 INFO  org.apache.kafka.clients.producer.internals.TransactionManager [] - [Producer clientId=producer-46] ProducerId set to 2045 with epoch 0
2025-09-29 22:45:07,770 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:45:07,770 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:45:07,770 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:45:07,770 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-46 unregistered
2025-09-29 22:45:07,789 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-45] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2025-09-29 22:45:07,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:45:07,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:45:07,855 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:45:07,855 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-45 unregistered
2025-09-29 22:45:07,855 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2025-09-29 22:45:07,855 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2025-09-29 22:45:08,294 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics scheduler closed
2025-09-29 22:45:08,294 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-09-29 22:45:08,294 INFO  org.apache.kafka.common.metrics.Metrics                      [] - Metrics reporters closed
2025-09-29 22:45:08,295 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.consumer for KafkaSource-1629489345713191071-0 unregistered
2025-09-29 22:45:08,295 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2025-09-29 22:45:08,296 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:203) ~[flink-connector-files-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:443) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:638) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:973) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763) [flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist-1.20.2.jar:1.20.2]
	at java.base/java.lang.Thread.run(Thread.java:840) [?:?]
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = transactions-data, partition = 0, leaderEpoch = 0, offset = 10207, CreateTime = 1759065375065, serialized key size = -1, serialized value size = 125, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@213b81a6).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:59) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: java.io.IOException: Failed to deserialize JSON '{"transaction_id": "TX000001", "user_id": "AC00128", "amount": 14.09, "timestamp": "11/04/23 16:29", "location": "San Diego"}'.
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:97) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
Caused by: org.apache.flink.formats.json.JsonParseException: Fail to deserialize at field: timestamp.
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$createRowConverter$43b82837$1(JsonParserToRowDataConverters.java:419) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserToRowDataConverters.lambda$wrapIntoNullableConverter$ca96cb8f$1(JsonParserToRowDataConverters.java:463) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:91) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.formats.json.JsonParserRowDataDeserializationSchema.deserialize(JsonParserRowDataDeserializationSchema.java:42) ~[flink-json-1.20.2.jar:1.20.2]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-dist-1.20.2.jar:1.20.2]
	at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.deserialize(DynamicKafkaDeserializationSchema.java:115) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[flink-connector-kafka-3.4.0-1.20.jar:3.4.0-1.20]
	... 14 more
2025-09-29 22:45:08,297 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 (49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22).
2025-09-29 22:45:08,298 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: transactions[1] -> Calc[2] -> alerts[3]: Writer -> alerts[3]: Committer (1/1)#22 49ecb251348a2a8a8c7d4007d7829cb5_cbc357ccb763df2852fee8c4fc7d55f2_0_22.
2025-09-29 22:45:50,955 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2025-09-29 22:45:50,956 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2025-09-29 22:45:50,956 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2025-09-29 22:45:50,957 INFO  org.apache.flink.runtime.state.TaskExecutorFileMergingManager [] - Shutting down TaskExecutorFileMergingManager.
2025-09-29 22:45:50,957 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2025-09-29 22:45:50,957 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
2025-09-29 22:45:50,959 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor pekko.tcp://flink@localhost:62144/user/rpc/taskmanager_0.
2025-09-29 22:45:50,960 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection 1a906028383dca151aadefbbe5f7ecaf.
2025-09-29 22:45:51,064 INFO  org.apache.pekko.actor.CoordinatedShutdown                   [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
2025-09-29 22:45:51,070 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
2025-09-29 22:45:51,070 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
2025-09-29 22:45:51,070 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.
2025-09-29 22:45:51,071 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.
2025-09-29 22:45:51,092 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.
2025-09-29 22:45:51,093 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.
